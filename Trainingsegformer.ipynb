{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrCarry123/OrganSegmentation/blob/main/Trainingsegformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD260CasQGHI",
        "outputId": "6cd91608-f5c8-4f5f-efc6-67cb56e8f38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.11-minimal libpython3.11-stdlib mailcap mime-support\n",
            "  python3.11-minimal\n",
            "Suggested packages:\n",
            "  python3.11-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.11-minimal libpython3.11-stdlib mailcap mime-support python3.11\n",
            "  python3.11-minimal\n",
            "0 upgraded, 6 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 5,783 kB of archives.\n",
            "After this operation, 21.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.11-minimal amd64 3.11.5-1+jammy1 [881 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.11-minimal amd64 3.11.5-1+jammy1 [2,347 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.11-stdlib amd64 3.11.5-1+jammy1 [1,907 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.11 amd64 3.11.5-1+jammy1 [622 kB]\n",
            "Fetched 5,783 kB in 1s (10.6 MB/s)\n",
            "Selecting previously unselected package libpython3.11-minimal:amd64.\n",
            "(Reading database ... 120895 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.11-minimal_3.11.5-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.11-minimal:amd64 (3.11.5-1+jammy1) ...\n",
            "Selecting previously unselected package python3.11-minimal.\n",
            "Preparing to unpack .../1-python3.11-minimal_3.11.5-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.11-minimal (3.11.5-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.11-stdlib:amd64.\n",
            "Preparing to unpack .../4-libpython3.11-stdlib_3.11.5-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.11-stdlib:amd64 (3.11.5-1+jammy1) ...\n",
            "Selecting previously unselected package python3.11.\n",
            "Preparing to unpack .../5-python3.11_3.11.5-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.11 (3.11.5-1+jammy1) ...\n",
            "Setting up libpython3.11-minimal:amd64 (3.11.5-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up python3.11-minimal (3.11.5-1+jammy1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.11-stdlib:amd64 (3.11.5-1+jammy1) ...\n",
            "Setting up python3.11 (3.11.5-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python3.11\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/Organsegmentation2/') #change this path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtwQVaE7WnCf",
        "outputId": "266378a5-fa97-4921-9481-7b4e2e5b9052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdHgtALTZj1X",
        "outputId": "1c902448-e8be-4034-86e5-6df9f4f3c690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Requirement already satisfied: keras==2.13.1 in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Collecting segmentation-models\n",
            "  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from segmentation-models)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting image-classifiers==1.0.0 (from segmentation-models)\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting efficientnet==1.0.0 (from segmentation-models)\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.23.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.9.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.11.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (23.2)\n",
            "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorflow==2.13.0\n",
        "!pip3 install keras==2.13.1\n",
        "!pip3 install -U segmentation-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRa6eVzRZlhZ",
        "outputId": "4b5f1e40-1879-4e5b-ce09-e9a8563dc0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SM_FRAMEWORK=tf.keras\n",
            "Segmentation Models: using `tf.keras` framework.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "\n",
        "%env SM_FRAMEWORK=tf.keras\n",
        "import segmentation_models as sm\n",
        "tf.keras.backend.set_image_data_format('channels_last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jO4N8omQGHJ"
      },
      "outputs": [],
      "source": [
        "import segmentation_models as sm\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "import nibabel as nib\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Conv2DTranspose, concatenate, BatchNormalization, Activation, MaxPool2D\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "# from nilearn.image import resample_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cot9iVCT6kn6"
      },
      "outputs": [],
      "source": [
        "from segformer import SegFormer_B3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjF9l_bIQGHL"
      },
      "outputs": [],
      "source": [
        "def np_load_and_preprocess(file_path):\n",
        "    original_image = cv2.imread(str(file_path.decode()))\n",
        "    # print('original:', original_image.shape)\n",
        "    # Resize the image to (32, 32) and convert it to (32, 32, 3)\n",
        "    resized_image = cv2.resize(original_image, (32, 32)).astype(np.float32)\n",
        "    original_label = cv2.imread(str(file_path.decode()).replace('images', 'labels').replace('volume', 'labels'))\n",
        "\n",
        "    # Resize the image to (32, 32) and convert it to (32, 32, 3)\n",
        "    resized_label = cv2.resize(original_label, (32, 32)).astype(np.float32)\n",
        "    resized_label[resized_label != 0] = 1\n",
        "    # print('check: ', resized_image.shape, resized_label.shape)\n",
        "    return resized_image, resized_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBjxO5d1QGHL"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess a single file\n",
        "def load_and_preprocess_file(file_path):\n",
        "    # Use tf.numpy_function to wrap the function and use NumPy functions inside TensorFlow\n",
        "    images, labels = tf.numpy_function(np_load_and_preprocess, [file_path], [tf.float32, tf.float32])\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbTR1VvKQGHL"
      },
      "outputs": [],
      "source": [
        "# change path\n",
        "# remove [:100] for training with full dataset\n",
        "volume_files = glob('/content/drive/MyDrive/Organsegmentation2/train_images/*.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(volume_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSdSL7QdJ8py",
        "outputId": "aea8631f-729a-4bb3-ecff-32660bbdded3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53760"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66MePv-1QGHL"
      },
      "outputs": [],
      "source": [
        "train_files, val_files = train_test_split(volume_files, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFU6zcViQGHL"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_files).shuffle(buffer_size=len(train_files))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(val_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pgrvr5VQGHM"
      },
      "outputs": [],
      "source": [
        "# Map the load_and_preprocess_file function to the dataset\n",
        "train_dataset = train_dataset.map(load_and_preprocess_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "val_dataset = val_dataset.map(load_and_preprocess_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# In[14]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx_Js_N7QGHM"
      },
      "outputs": [],
      "source": [
        "# Shuffle, batch, cache, and prefetch the dataset\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_files))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4gc_O8DQGHM"
      },
      "outputs": [],
      "source": [
        "val_dataset = val_dataset.shuffle(buffer_size=len(val_files))\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.cache()\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zShrpWVQGHM"
      },
      "outputs": [],
      "source": [
        "opt = Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yfhbJ58QGHM"
      },
      "outputs": [],
      "source": [
        "def DiceBCELoss(targets, inputs):\n",
        "    smooth = 1e-6\n",
        "    inputs = tf.keras.backend.flatten(inputs)\n",
        "    targets = tf.keras.backend.flatten(targets)\n",
        "\n",
        "    BCE = tf.keras.losses.binary_crossentropy(targets, inputs)\n",
        "    intersection = tf.reduce_sum(targets * inputs)\n",
        "    dice_loss = 1.0 - (2.0 * intersection + smooth) / (tf.reduce_sum(targets) + tf.reduce_sum(inputs) + smooth)\n",
        "    Dice_BCE = BCE + dice_loss\n",
        "    return Dice_BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fgukyKRQGHM"
      },
      "outputs": [],
      "source": [
        "model = SegFormer_B3(input_shape = (32, 32, 3), num_classes = 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A5VvHL7QGHN",
        "outputId": "2d6eeafc-ff45-48da-e0d1-b35b88ab2a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " mix_vision_transformer (Mi  [(None, 8, 8, 64),           4407212   ['input_1[0][0]']             \n",
            " xVisionTransformer)          (None, 4, 4, 128),          8                                       \n",
            "                              (None, 2, 2, 320),                                                  \n",
            "                              (None, 1, 1, 512)]                                                  \n",
            "                                                                                                  \n",
            " seg_former_head (SegFormer  (None, 8, 8, 3)              3154179   ['mix_vision_transformer[0][0]\n",
            " Head)                                                              ',                            \n",
            "                                                                     'mix_vision_transformer[0][1]\n",
            "                                                                    ',                            \n",
            "                                                                     'mix_vision_transformer[0][2]\n",
            "                                                                    ',                            \n",
            "                                                                     'mix_vision_transformer[0][3]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " resize_layer (ResizeLayer)  (None, 32, 32, 3)            0         ['seg_former_head[0][0]']     \n",
            "                                                                                                  \n",
            " tf.nn.softmax (TFOpLambda)  (None, 32, 32, 3)            0         ['resize_layer[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 47226307 (180.15 MB)\n",
            "Trainable params: 47224771 (180.15 MB)\n",
            "Non-trainable params: 1536 (6.00 KB)\n",
            "__________________________________________________________________________________________________\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "model.summary()\n",
        "num_layers = len(model.layers)\n",
        "print(num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHRFXQqDltss",
        "outputId": "4a081ac0-9dda-4ec5-d2cb-d70ee5d1e235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txaAy8ixQGHN",
        "outputId": "bf3e07c3-9565-47b6-e41d-bc023b3f6cab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7832c8f356c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7832c8f356c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1344it [2:03:13,  5.50s/it]\n",
            "336it [23:17,  4.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Training Loss: 1.2937, Validation Loss: 1.2943, Time: 8790.78 seconds\n",
            "\n",
            "Start of epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [43:27,  1.94s/it]\n",
            "336it [03:55,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 1.2933, Validation Loss: 1.2938, Time: 2843.07 seconds\n",
            "\n",
            "Start of epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [42:58,  1.92s/it]\n",
            "336it [03:55,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Training Loss: 1.2932, Validation Loss: 1.2934, Time: 2813.94 seconds\n",
            "\n",
            "Start of epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [43:06,  1.92s/it]\n",
            "336it [03:55,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Training Loss: 1.2967, Validation Loss: 1.3207, Time: 2821.44 seconds\n",
            "\n",
            "Start of epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [42:43,  1.91s/it]\n",
            "336it [03:54,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Training Loss: 1.2932, Validation Loss: 1.2934, Time: 2798.37 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "from tqdm import tqdm\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop\n",
        "    for step, (x, y) in tqdm(enumerate(train_dataset)):\n",
        "        # print(x.shape, y.shape)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            pred = model(x, training=True)\n",
        "            loss_value = DiceBCELoss(y, pred)\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "    for step, (val_x, val_y) in tqdm(enumerate(val_dataset)):\n",
        "        val_pred = model(val_x, training=False)\n",
        "        val_loss += DiceBCELoss(val_y, val_pred)\n",
        "        val_steps += 1\n",
        "    avg_val_loss = val_loss / val_steps\n",
        "\n",
        "    print(\"Epoch: %d, Training Loss: %.4f, Validation Loss: %.4f, Time: %.2f seconds\" % (epoch, loss_value.numpy(), avg_val_loss, time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('sm10.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as1FUIjkL4n-",
        "outputId": "aa3b8cd4-aea6-4e9d-8067-67e53c755e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}