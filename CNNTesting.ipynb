{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrCarry123/OrganSegmentation/blob/main/CNNTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGxoODlQPZcb",
        "outputId": "e90ac5a3-9c4a-4b48-ec1b-8343cb7f6354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/Organsegmentation2/') #change this path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MEiP4xFDWZhX",
        "outputId": "9afc6628-6ea3-4caa-c9e6-2a6a5e0fb0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.9.3\n",
            "  Downloading tensorflow-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.3)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.3)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.3)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (23.2)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.3)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.3)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (0.34.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.3)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.3) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.3) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.3)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.3)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.3)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.3) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.3) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.5.26\n",
            "    Uninstalling flatbuffers-23.5.26:\n",
            "      Successfully uninstalled flatbuffers-23.5.26\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.1\n",
            "    Uninstalling tensorboard-data-server-0.7.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.3 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras==2.9.0 in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Collecting segmentation-models\n",
            "  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from segmentation-models)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting image-classifiers==1.0.0 (from segmentation-models)\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting efficientnet==1.0.0 (from segmentation-models)\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.23.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.9.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.11.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (23.2)\n",
            "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"
          ]
        }
      ],
      "source": [
        " !pip3 install tensorflow==2.9.3\n",
        " !pip3 install keras==2.9.0\n",
        " !pip3 install -U segmentation-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1T8ftjcWcxd",
        "outputId": "70c40e3f-5ba3-473d-984a-40f9b7b48596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SM_FRAMEWORK=tf.keras\n",
            "Segmentation Models: using `tf.keras` framework.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "\n",
        "%env SM_FRAMEWORK=tf.keras\n",
        "import segmentation_models as sm\n",
        "tf.keras.backend.set_image_data_format('channels_last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0IwHtH5PZcd"
      },
      "outputs": [],
      "source": [
        "import segmentation_models as sm\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "import nibabel as nib\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Conv2DTranspose, concatenate, BatchNormalization, Activation, MaxPool2D\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "# from nilearn.image import resample_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZGd1xTuPN9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzAriq4qPZce"
      },
      "outputs": [],
      "source": [
        "# Uncomment to select the model\n",
        "#from resnet50_unet import build_resnet50_unet\n",
        "#from vgg16_unet import build_vgg16_unet\n",
        "from vgg19_unet import build_vgg19_unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTmPyzf-PZce"
      },
      "outputs": [],
      "source": [
        "# names = os.listdir('/hdd_storage/data/sumitk/ct_org/train_labels/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU6tzdDJPZcf"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BufbadmVPZcf"
      },
      "outputs": [],
      "source": [
        "def np_load_and_preprocess(file_path):\n",
        "    original_image = cv2.imread(str(file_path.decode()))\n",
        "    # print('original:', original_image.shape)\n",
        "    # Resize the image to (32, 32) and convert it to (32, 32, 3)\n",
        "    resized_image = cv2.resize(original_image, (32, 32)).astype(np.float32)\n",
        "    original_label = cv2.imread(str(file_path.decode()).replace('images', 'labels').replace('volume', 'labels'))\n",
        "\n",
        "    # Resize the image to (32, 32) and convert it to (32, 32, 3)\n",
        "    resized_label = cv2.resize(original_label, (32, 32)).astype(np.float32)\n",
        "    resized_label[resized_label != 0] = 1\n",
        "    # print('check: ', resized_image.shape, resized_label.shape)\n",
        "    return resized_image, resized_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF9phJKjPZcg"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess a single file\n",
        "def load_and_preprocess_file(file_path):\n",
        "    # Use tf.numpy_function to wrap the function and use NumPy functions inside TensorFlow\n",
        "    images, labels = tf.numpy_function(np_load_and_preprocess, [file_path], [tf.float32, tf.float32])\n",
        "    # images = preprocess_input(images)\n",
        "    # images = images[:, :,63]\n",
        "    # labels = labels[:, :,63]\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fmL8ab9PZcg"
      },
      "outputs": [],
      "source": [
        "# change path\n",
        "# remove [:100] for training with full dataset\n",
        "volume_files = glob('/content/drive/MyDrive/Organsegmentation2/train_images/*.png')\n",
        "# random.shuffle(volume_files)\n",
        "#volume_files = volume_files[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(volume_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzj3fuq_Rqy2",
        "outputId": "876cb002-f0ba-43ad-a6e9-390cdfe006db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53760"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZbIoyWPPZcg"
      },
      "outputs": [],
      "source": [
        "train_files, val_files = train_test_split(volume_files, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nqdTOpNPZcg"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_files).shuffle(buffer_size=len(train_files))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(val_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD4kux2-PZcg"
      },
      "outputs": [],
      "source": [
        "# Map the load_and_preprocess_file function to the dataset\n",
        "train_dataset = train_dataset.map(load_and_preprocess_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "val_dataset = val_dataset.map(load_and_preprocess_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# In[14]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOKotJOVPZcg"
      },
      "outputs": [],
      "source": [
        "# Shuffle, batch, cache, and prefetch the dataset\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_files))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "#train_dataset = train_dataset.cache()\n",
        "#train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O1Vkw8fPZch"
      },
      "outputs": [],
      "source": [
        "# for step, (x,y) in enumerate(train_dataset):\n",
        "#     print(x)\n",
        "#     print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "671tAP8MPZch"
      },
      "outputs": [],
      "source": [
        "val_dataset = val_dataset.shuffle(buffer_size=len(val_files))\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "#val_dataset = val_dataset.cache()\n",
        "#val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEcrT97QPZch"
      },
      "outputs": [],
      "source": [
        "# Uncomment to use Custom U-Net model\n",
        "\n",
        "# def unet(label_num=4, pretrained_weights=None):\n",
        "\n",
        "#     inputs = Input((32, 32, 4))\n",
        "\n",
        "#     '''downsample'''\n",
        "#     conv1 = Conv2D(8, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
        "#     batc1 = BatchNormalization(axis=-1)(conv1)\n",
        "#     acti1 = Activation('relu')(batc1)\n",
        "#     conv2 = Conv2D(16, 3 , padding='same', kernel_initializer='he_normal')(acti1)\n",
        "#     batc2 = BatchNormalization(axis=-1)(conv2)\n",
        "#     acti2 = Activation('relu')(batc2)\n",
        "#     maxp1 = MaxPooling2D(2)(acti2)\n",
        "\n",
        "\n",
        "#     conv3 = Conv2D(16, 3, padding='same', kernel_initializer='he_normal')(maxp1)\n",
        "#     batc3 = BatchNormalization(axis=-1)(conv3)\n",
        "#     acti3 = Activation('relu')(batc3)\n",
        "#     conv4 = Conv2D(32, 3, padding='same', kernel_initializer='he_normal')(acti3)\n",
        "#     batc4 = BatchNormalization(axis=-1)(conv4)\n",
        "#     acti4 = Activation('relu')(batc4)\n",
        "#     maxp2 = MaxPooling2D(2)(acti4)\n",
        "\n",
        "\n",
        "#     conv5 = Conv2D(32, 3, padding='same', kernel_initializer='he_normal')(maxp2)\n",
        "#     batc5 = BatchNormalization(axis=-1)(conv5)\n",
        "#     acti5 = Activation('relu')(batc5)\n",
        "#     conv6 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(acti5)\n",
        "#     batc6 = BatchNormalization(axis=-1)(conv6)\n",
        "#     acti6 = Activation('relu')(batc6)\n",
        "#     maxp3 = MaxPooling2D(2)(acti6)\n",
        "\n",
        "#     conv7 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(maxp3)\n",
        "#     batc7 = BatchNormalization(axis=-1)(conv7)\n",
        "#     acti7 = Activation('relu')(batc7)\n",
        "#     conv8 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(acti7)\n",
        "#     batc8 = BatchNormalization(axis=-1)(conv8)\n",
        "#     acti8 = Activation('relu')(batc8)\n",
        "\n",
        "\n",
        "#     '''upsample'''\n",
        "#     upsa1 = UpSampling2D(2)(acti8)\n",
        "#     # print('upsam1 shape: ', upsam1.shape)\n",
        "#     # Calculate the cropping amount for conv6\n",
        "#     # crop_amount = 1  # Adjust this calculation as needed based on your specific requirements\n",
        "\n",
        "#     # # # Crop conv6 to match the dimensions of upsa1\n",
        "#     # # cropped_conv6 = Cropping3D(cropping=((0, 0), (0, 0), (crop_amount, crop_amount)))(conv6)\n",
        "#     # sliced_conv6 = conv6[:, crop_amount:-crop_amount, crop_amount:-crop_amount, crop_amount:-crop_amount, :]\n",
        "\n",
        "#     # print(tf.shape(conv6[:, :, :, :16, :]))\n",
        "#     # print(tf.shape(upsa1))\n",
        "#     merg1 = Concatenate(axis=-1)([conv6, upsa1])\n",
        "\n",
        "#     # merg1 = Concatenate(axis=-1)([conv6[:, :, :, :16, :], upsa1])\n",
        "#     conv9 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(merg1)\n",
        "#     batc9 = BatchNormalization(axis=-1)(conv9)\n",
        "#     acti9 = Activation('relu')(batc9)\n",
        "#     conv10 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(acti9)\n",
        "#     batc10 = BatchNormalization(axis=-1)(conv10)\n",
        "#     acti10 = Activation('relu')(batc10)\n",
        "\n",
        "#     upsa2 = UpSampling2D(2)(acti10)\n",
        "#     merg2 = Concatenate(axis=-1)([conv4, upsa2])\n",
        "#     conv11 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(merg2)\n",
        "#     batc11 = BatchNormalization(axis=-1)(conv11)\n",
        "#     acti11 = Activation('relu')(batc11)\n",
        "#     conv12 = Conv2D(32, 3, padding='same', kernel_initializer='he_normal')(acti11)\n",
        "#     batc12 = BatchNormalization(axis=-1)(conv12)\n",
        "#     acti12 = Activation('relu')(batc12)\n",
        "#     # print(tf.shape(acti12))\n",
        "#     upsa3 = UpSampling2D(2)(acti12)\n",
        "#     # print(tf.shape(upsa3))\n",
        "#     # print(tf.shape(conv2))\n",
        "#     merg3 = Concatenate(axis=-1)([conv2, upsa3])\n",
        "#     conv13 = Conv2D(16, 3, padding='same', kernel_initializer='he_normal')(merg3)\n",
        "#     batc13 = BatchNormalization(axis=-1)(conv13)\n",
        "#     acti13 = Activation('relu')(batc13)\n",
        "#     conv14 = Conv2D(16, 3, padding='same', kernel_initializer='he_normal')(acti13)\n",
        "#     convol = Conv2D(label_num, 1, padding='same',activation='sigmoid')(conv14)\n",
        "\n",
        "\n",
        "#     model = Model(inputs=inputs, outputs=convol)\n",
        "#     #model.compile(optimizer=Adam(lr=1e-4), loss=dice_coef_loss, metrics=[dice_coef])\n",
        "#     # model.compile(optimizer=Adam(lr=1e-4), loss=dice_coef_loss, metrics=[dice_coef])\n",
        "#     #model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=[dice_coef])\n",
        "#     #model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # model.summary()\n",
        "\n",
        "#     if (pretrained_weights):\n",
        "#         model.load_weights(pretrained_weights)\n",
        "\n",
        "#     return model\n",
        "# model = unet(label_num=4, pretrained_weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4gmsIwzPZch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8088d535-3080-4e40-a451-1361c934c1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to select the model\n",
        "#model = build_resnet50_unet((32, 32, 3))\n",
        "#model = build_vgg16_unet((32, 32, 3))\n",
        "model = build_vgg19_unet((32, 32, 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "num_layers = len(model.layers)\n",
        "print(num_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFzs0Eqzl9yi",
        "outputId": "7b9ecf7a-2c09-4ac5-f806-3e47b13b15e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"VGG19_U-Net\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 32, 32, 64)   1792        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 32, 32, 64)   36928       ['block1_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, 16, 16, 64)   0           ['block1_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, 16, 16, 128)  73856       ['block1_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, 16, 16, 128)  147584      ['block2_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 8, 8, 128)    0           ['block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, 8, 8, 256)    295168      ['block2_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, 8, 8, 256)    590080      ['block3_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, 8, 8, 256)    590080      ['block3_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv4 (Conv2D)          (None, 8, 8, 256)    590080      ['block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 4, 4, 256)    0           ['block3_conv4[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, 4, 4, 512)    1180160     ['block3_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv4 (Conv2D)          (None, 4, 4, 512)    2359808     ['block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 2, 2, 512)    0           ['block4_conv4[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, 2, 2, 512)    2359808     ['block4_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block5_conv2 (Conv2D)          (None, 2, 2, 512)    2359808     ['block5_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv3 (Conv2D)          (None, 2, 2, 512)    2359808     ['block5_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv4 (Conv2D)          (None, 2, 2, 512)    2359808     ['block5_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2DTran  (None, 4, 4, 512)   1049088     ['block5_conv4[0][0]']           \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 4, 4, 1024)   0           ['conv2d_transpose_4[0][0]',     \n",
            "                                                                  'block4_conv4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 4, 4, 512)    4719104     ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 4, 4, 512)   2048        ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 4, 4, 512)    0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 4, 4, 512)    2359808     ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 4, 4, 512)   2048        ['conv2d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 4, 4, 512)    0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2DTran  (None, 8, 8, 256)   524544      ['activation_9[0][0]']           \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 8, 8, 512)    0           ['conv2d_transpose_5[0][0]',     \n",
            "                                                                  'block3_conv4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 8, 8, 256)    1179904     ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_6 (Conv2DTran  (None, 16, 16, 128)  131200     ['activation_11[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 16, 16, 256)  0           ['conv2d_transpose_6[0][0]',     \n",
            "                                                                  'block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 128)  295040      ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 128)  512        ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 128)  147584      ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 128)  512        ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_7 (Conv2DTran  (None, 32, 32, 64)  32832       ['activation_13[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 32, 32, 128)  0           ['conv2d_transpose_7[0][0]',     \n",
            "                                                                  'block1_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 32, 32, 64)   73792       ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 32, 32, 64)  256         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 32, 32, 64)   36928       ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 32, 32, 64)  256         ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 32, 32, 3)    195         ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,172,163\n",
            "Trainable params: 31,168,323\n",
            "Non-trainable params: 3,840\n",
            "__________________________________________________________________________________________________\n",
            "54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwBU1H_pPZch"
      },
      "outputs": [],
      "source": [
        "opt = Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htLkkPB-PZch"
      },
      "outputs": [],
      "source": [
        "def DiceBCELoss(targets, inputs):\n",
        "    smooth = 1e-6\n",
        "    inputs = tf.keras.backend.flatten(inputs)\n",
        "    targets = tf.keras.backend.flatten(targets)\n",
        "\n",
        "    BCE = tf.keras.losses.binary_crossentropy(targets, inputs)\n",
        "    intersection = tf.reduce_sum(targets * inputs)\n",
        "    dice_loss = 1.0 - (2.0 * intersection + smooth) / (tf.reduce_sum(targets) + tf.reduce_sum(inputs) + smooth)\n",
        "    Dice_BCE = BCE + dice_loss\n",
        "    return Dice_BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijhMCYNPPZch",
        "outputId": "07157827-b82d-41e7-cee2-736b86932268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [41:14,  1.84s/it]\n",
            "336it [09:29,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Training Loss: 0.3444, Validation Loss: 0.2565, Time: 3044.71 seconds\n",
            "\n",
            "Start of epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:07,  7.16it/s]\n",
            "336it [00:29, 11.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.1227, Validation Loss: 0.1350, Time: 217.09 seconds\n",
            "\n",
            "Start of epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:07,  7.16it/s]\n",
            "336it [00:29, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Training Loss: 0.0827, Validation Loss: 0.1099, Time: 217.47 seconds\n",
            "\n",
            "Start of epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:06,  7.19it/s]\n",
            "336it [00:29, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Training Loss: 0.2232, Validation Loss: 0.0987, Time: 216.11 seconds\n",
            "\n",
            "Start of epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:06,  7.22it/s]\n",
            "336it [00:29, 11.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Training Loss: 0.3199, Validation Loss: 0.0829, Time: 215.69 seconds\n",
            "\n",
            "Start of epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:07,  7.18it/s]\n",
            "336it [00:29, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Training Loss: 0.0613, Validation Loss: 0.0765, Time: 216.21 seconds\n",
            "\n",
            "Start of epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:06,  7.21it/s]\n",
            "336it [00:29, 11.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Training Loss: 0.0599, Validation Loss: 0.0693, Time: 215.76 seconds\n",
            "\n",
            "Start of epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:06,  7.20it/s]\n",
            "336it [00:29, 11.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Training Loss: 0.0598, Validation Loss: 0.0673, Time: 216.02 seconds\n",
            "\n",
            "Start of epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:06,  7.19it/s]\n",
            "336it [00:29, 11.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Training Loss: 0.0679, Validation Loss: 0.0616, Time: 216.52 seconds\n",
            "\n",
            "Start of epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:06,  7.21it/s]\n",
            "336it [00:29, 11.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Training Loss: 0.0581, Validation Loss: 0.0571, Time: 215.39 seconds\n",
            "\n",
            "Start of epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:05,  7.23it/s]\n",
            "336it [00:29, 11.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Training Loss: 0.0706, Validation Loss: 0.0620, Time: 214.86 seconds\n",
            "\n",
            "Start of epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:05,  7.24it/s]\n",
            "336it [00:28, 11.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Training Loss: 0.0470, Validation Loss: 0.0505, Time: 214.61 seconds\n",
            "\n",
            "Start of epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:05,  7.24it/s]\n",
            "336it [00:28, 11.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Training Loss: 0.0562, Validation Loss: 0.0481, Time: 214.67 seconds\n",
            "\n",
            "Start of epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:06,  7.22it/s]\n",
            "336it [00:29, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Training Loss: 0.0431, Validation Loss: 0.0478, Time: 215.38 seconds\n",
            "\n",
            "Start of epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1344it [03:05,  7.23it/s]\n",
            "336it [00:29, 11.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Training Loss: 0.0367, Validation Loss: 0.0447, Time: 215.13 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 15\n",
        "from tqdm import tqdm\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop\n",
        "    for step, (x, y) in tqdm(enumerate(train_dataset)):\n",
        "        # print(x.shape, y.shape)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            pred = model(x, training=True)\n",
        "            loss_value = DiceBCELoss(y, pred)\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "    for step, (val_x, val_y) in tqdm(enumerate(val_dataset)):\n",
        "        val_pred = model(val_x, training=False)\n",
        "        val_loss += DiceBCELoss(val_y, val_pred)\n",
        "        val_steps += 1\n",
        "    avg_val_loss = val_loss / val_steps\n",
        "\n",
        "    print(\"Epoch: %d, Training Loss: %.4f, Validation Loss: %.4f, Time: %.2f seconds\" % (epoch, loss_value.numpy(), avg_val_loss, time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsgEnduDPZch",
        "outputId": "dc37b514-3a70-4055-ba8c-9877b39c1b71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model.save('sm6.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlM6V67fPZch"
      },
      "outputs": [],
      "source": [
        "# !jupyter nbconvert --to  script 'sm.ipynb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk6GnzLaPZch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psa7mYDyPZch"
      },
      "outputs": [],
      "source": [
        "# # load your data\n",
        "\n",
        "\n",
        "# # preprocess input\n",
        "# x_train = preprocess_input(x_train)\n",
        "# x_val = preprocess_input(x_val)\n",
        "\n",
        "# # define model\n",
        "# model = sm.Unet('resnet34', classes=7, activation='softmax')\n",
        "# model.compile(\n",
        "#     'Adam',\n",
        "#     loss=sm.losses.bce_jaccard_loss,\n",
        "#     metrics=[sm.metrics.iou_score],\n",
        "# )\n",
        "\n",
        "# # fit model\n",
        "# # if you use data generator use model.fit_generator(...) instead of model.fit(...)\n",
        "# # more about `fit_generator` here: https://keras.io/models/sequential/#fit_generator\n",
        "# model.fit(\n",
        "#    x=x_train,\n",
        "#    y=y_train,\n",
        "#    batch_size=16,\n",
        "#    epochs=100,\n",
        "#    validation_data=(x_val, y_val),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzYCNJkhPZci"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}